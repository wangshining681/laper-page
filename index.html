<html>
<!-- happy -->
<head>
    <link rel="StyleSheet" href="style.css" type="text/css" media="all">
    <title>SeCap: Self-Calibrating and Adaptive Prompts for Cross-view Person Re-Identification in Aerial-Ground Networks</title>
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
</head>
<body>
    <br>
    <div class="center-div" align="center">
        <span style="font-size:28px">SeCap: Self-Calibrating and Adaptive Prompts for Cross-view Person Re-Identification in Aerial-Ground Networks</span>
    </div>

    <br>
    <table align="center" width="1000px">
        <tbody>
            <tr>
                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="https://scholar.google.com" target="_blank">Shining Wang</a></span>
                    </div>
                </td>
                
                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="https://scholar.google.com/" target="_blank">Yunlong Wang</a></span>
                    </div>
                </td>
                
                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="https://scholar.google.com/citations?user=cRmHbWkAAAAJ&hl=en" target="_blank">Ruiqi Wu</a></span>
                    </div>
                </td>

                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="https://scholar.google.com/citations?user=a-RAVckAAAAJ&hl=en" target="_blank">Bingliang Jiao</a></span>
                    </div>
                </td>
                
                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="https://wxwangiris.github.io/" target="_blank">Wenxuan Wang</a></span>
                    </div>
                </td>

                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="https://scholar.google.com/citations?user=aPLp7pAAAAAJ&hl=en" target="_blank">Peng Wang</a></span>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>
    <br>
    <table align="center" width="1200px">
        <tbody>
            <tr>
                <td align="center" width="1200px">
                    <div class="center-div">
                        <span style="font-size:18px">School of Computer Science, Northwestern Polytechnical University, China </span>
                    </div>
                    <div class="center-div">
                        <span style="font-size:18px">Ningbo Institute, Northwestern Polytechnical University, China</span>
                    </div>
                    <div class="center-div">
                        <span style="font-size:18px">Shenzhen Research Institute, Northwestern Polytechnical University, China</span>
                    </div>
                    <div class="center-div">
                        <span style="font-size:18px">National Engineering Laboratory for integrated Aero-Space-Ground-Ocean, Xi'an, Shaanxi, China</span>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>

    <br>

    <hr>
    <div class="center-div" align="center">
        <h2>Abstract</h2>
    </div>
    <table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> 
                When discussing the Aerial-Ground Person Re-identification (AGPReID) task, 
                we face the main challenge of the significant appearance variations caused by different viewpoints, 
                making identity matching difficult. To address this issue, previous methods attempt to reduce the 
                differences between viewpoints by critical attributes and decoupling the viewpoints. 
                While these methods can mitigate viewpoint differences to some extent, they still face two main issues: 
                (1) difficulty in handling viewpoint diversity and (2) neglect of the contribution of local features. 
                To effectively address these challenges, we design and implement the Self-Calibrating and Adaptive Prompt 
                (SeCap) method for the AGPReID task. The core of this framework relies on the Prompt Re-calibration Module 
                (PRM), which adaptively re-calibrates prompts based on the input. Combined with the Local Feature Refinement 
                Module (LFRM), SeCap can extract view-invariant features from local features for AGPReID. Meanwhile, given the 
                current scarcity of datasets in the AGPReID field, we further contribute two real-world Large-scale Aerial-Ground 
                Person Re-Identification datasets, LAGPeR and G2APS-ReID. The former is collected and annotated by us independently, 
                covering $4,231$ unique identities and containing $63,841$ high-quality images; the latter is reconstructed from the 
                person search dataset G2APS. Through extensive experiments on AGPReID datasets, we demonstrate that SeCap is a 
                feasible and effective solution for the AGPReID task.
            </div>
        </td>
       
    </table>
    <br><br>
    <hr>
    <div class="center-div" align="center">
        <h2>Dataset Download</h2>
    </div>
    <div align="center" width="1000px">
        <h3>Data Release Agreement</h3>
    </div>
    <table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> 
                Dataset records are made available to researchers only after the receipt and acceptance of
                 a completed and signed Database Release Agreement.
            </div>
            <span style="font-size:20px">
                <a href="./Restrictions for The Use of LAGPeR Dataset.pdf">[Data Release Protocol]</a>
                <!-- <a href="https://drive.google.com/open?id=1wpqZCYh7gGKz897C-hQbgR7CHbJdBGxg">[Dataset Google Drive]</a>
                <a href="https://pan.baidu.com/s/1jhBqJeXShYyChfp1K3RcCw">[Dataset Baidu Drive](password:n2zw)</a> -->
            </span>
            <div>
            <span style="font-size:18px"> 
                Please submit requests for the dataset unless otherwise indicated: wangshining_wsn@163.com or wangshining@mail.nwpu.edu.cn
            </div>
        </td>
       
    </table>
    <br><br>
    <hr>
    <div class="center-div" align="center">
        <h2>Large-scale Aerial-Ground Person Re-identification(LAGPeR) datasets</h2>
    </div>
    <table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> 
                To expand the datasets available for the AGPReID task, 
                we contribute the LAGPeR and G2APS-ReID datasets. 
                The LAGPeR dataset is independently collected, annotated, and partitioned by us,
                 and it includes data from 21 cameras, 7 scenes, and 3 perspectives 
                 (with ground perspectives divided into oblique and frontal views).  
                 The G2APS-ReID dataset is reconstructed from the large-scale person search dataset G2APS.
                  Since the original G2APS dataset only considers retrieval tasks from ground to aerial view,
                   which do not fully meet the requirements of the AGPReID task, we re-partition the G2APS.
            </div>
        </td>
       
    </table>
    <br><br>

    <hr>
    <div class="center-div" align="center">
        <h2>Self-Calibrating and Adaptive Prompt (SeCap) method</h2>
    </div>

    <table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px">We proposes an AGPReID framework named SeCap, 
                which self-calibrates and adaptively generates prompts based on the inputs 
                for cross-view person re-identification. This framework adopts an encoder-decoder 
                transformer architecture. The encoder employs the View Decoupling Transformer (VDT) for 
                viewpoint decoupling, while the decoder further decodes local features using the view-invariant features. 
                Specifically, the decoder comprises the Prompt Re-calibration Module (PRM) and the Local Feature Refinement
                 Module (LPRM). To address the challenge of viewpoint diversity, we design the PRM 
                 to re-calibrate prompts based on the input adaptively. It dynamically generates and self-calibrates 
                 prompts that closely align with the current viewpoint, thus adapt to different viewpoints. To fully 
                 leverage the role of local features, we design the LPRM for local feature refinement. 
                 This module uses re-calibrated prompts and employs the to-way attention mechanism to synchronously 
                 update various features, thereby learning view-invariant information from local features.
            </div>
        </td>
    </table>

    <br>
    <table align="center">
        <tbody>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <a href="#"><img src="imgs/FIG_FRAMEWORK.png" width="1000px"></a><br>
                    </div>
                </td>
            </tr>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <span style="font-size:16px"><i>SeCap Overall Framework</i>
                        </span></div>
                </td>
            </tr>
        </tbody>
    </table>
	<br>

    <table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> The overall framework of SeCap adopts an encoder-decoder transformer architecture.
                 The encoder is the view decoupling transformer (VDT).
                  In contrast to the conventional ViT, our approach incorporates the View token and performs hierarchical
                   decoupling of the Cls token at each layer, effectively segregating view-related and 
                   view-invariant features within the Cls token, while extracting local features from the input.
                    The decoder comprises the Prompt Re-calibration Module (PRM) and the Local Feature Refinement Module (LFRM).
                     The PRM adaptively generates and re-calibrates prompts for different viewpoints based on the current viewpoint
                      information. Concurrently, the LFRM utilizes the re-calibrated prompts from the PRM to decode the local features.
            </div>
        </td>
    </table>
    <br><br>

	
	<!-- <hr> 
	<div class="center-div" align="center">
        <h2>Extended Makeup Face Dataset (EMFD)</h2>
    </div>

    <table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> To facilitate this study, we assembled a new makeup face verfication database of 1102 face images 
			that is 551 pairs of individuals. Each pair has one makeup and one non-makeup face images of the same individual. 
			All face images are collected from the Internet with text information about makeup or non-makeup. So the 
			labels of makeup and non-makeup, and the facial identities are collected together with the face images from 
			the Internet. This dataset have the facial images of large areas of acne, glasses occlusion, head posture changes and
			so on. Some face examples from our database are shown in below.
            </div>
        </td>
    </table>

    <br>
    <table align="center">
        <tbody>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <a href="#"><img src="images/cvpr2020_makeup/cvpr2020_expand_dataset.png" width="600px"></a><br>
                    </div>
                </td>
            </tr>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <span style="font-size:16px"><i>Extended Makeup Face Dataset</i>
                        </span></div>
                </td>
            </tr>
        </tbody>
    </table> 
	
	<br>
	<table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> The file name in the folder is xxx_m or xxx_n, where xxx indicates the xxxth person, 
			"m" indicates the image with makeup, and "n" indicates the image without makeup. 
			This dataset is for non-commercial reseach purposes (such as academic research) only.
			<a href="https://drive.google.com/open?id=1wpqZCYh7gGKz897C-hQbgR7CHbJdBGxg">[Dataset Google Drive]</a>
            <a href="https://pan.baidu.com/s/1jhBqJeXShYyChfp1K3RcCw">[Dataset Baidu Drive](password:n2zw)</a>			
            </div>
        </td>
    </table>
	<br><br> -->

    <hr>
    <div class="center-div" align="center">
        <h2 id="paper">Paper and Code</h2>
    </div>
    <table align="center">

        <tbody>
            <tr>
                <td>
                    <img class="layered-paper-big" style="height:250px" src="imgs/paper.png">
                </td>
                <td> &nbsp &nbsp </td>
                <td>
					<b><span style="display:inline-block;width:600px;font-size:14pt">  SeCap: Self-Calibrating and Adaptive Prompts for Cross-view Person Re-Identification in Aerial-Ground Networks
                    </span></b>
                    <br><br>
                    <span style="font-size:14pt"> Shining Wang<sup>*</sup>, Yunlong Wang<sup>*</sup>, Ruiqi Wu, Bingliang Jiao, Wenxuan Wang<sup>†</sup>, Peng Wang</span>
                    <br><br>
                    <span style="font-size:10pt"> <sup>*</sup>Equal contribution; <sup>†</sup>Corresponding author.</span>
                    <br><br>
                    <span style="font-size:14pt"> CVPR 2025, HighLight</span>
                    <br><br>
                    <span style="font-size:14px">
                        <a href="https://arxiv.org/abs/2503.06965">[Paper]</a>
                        <a href="./BibTex.bib">[Bibtex]</a>
                        <a href="https://arxiv.org/abs/2503.06965">[arXiv]</a>
                        <a href="https://github.com/wangshining681/SeCap-AGPReID">[GitHub]</a>
                        <!-- <a href="https://drive.google.com/open?id=1wpqZCYh7gGKz897C-hQbgR7CHbJdBGxg">[Dataset Google Drive]</a>
                        <a href="https://pan.baidu.com/s/1jhBqJeXShYyChfp1K3RcCw">[Dataset Baidu Drive](password:n2zw)</a> -->
                    </span>
                </td>
            </tr>
        </tbody>
    </table>
    <br><br>
	
	
	<hr> 
	<div class="center-div" align="center">
        <h2>Results</h2>
    </div>

    <br>
    <table align="center">
        <tbody>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <a href="#"><img src="imgs/FIG_RESULT.png" width="800px"></a><br>
                    </div>
                </td>
            </tr>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <span style="font-size:16px"><i>Comparison Results</i>
                        </span></div>
                </td>
            </tr>
        </tbody>
    </table>
	<br><br>
    <table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> The experiment results of our SeCap and other methods under various test modes of LAGPeR and G2APS-ReID datasets. 
                Summarily, our proposed SeCap outperforms other state-of-the-art methods on both two datasets. 
            </div>
        </td>
    </table>
    <br><br>

	<!-- <br>
    <table align="center">
        <tbody>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <a href="#"><img src="imgs/FIG_VIS.png" width="700px"></a><br>
                    </div>
                </td>
            </tr>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <span style="font-size:16px"><i>Visualization Results</i>
                        </span></div>
                </td>
            </tr>
        </tbody>
    </table>
	<br><br>
    <table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> The visualizations results of attention maps of our MPL module and baseline model. 
                From the second column in each case, we could find that the baseline model tends only to capture the explicit correspondence between different modality inputs. 
                Such as only focusing on the upper dress part while ignoring the skirt part in case (a). 
                As for our MPL module, with the carefully designed modality-specific prompts, it could effectively adapt to and make use of the modality-specific information. 
                This enables our MPL model to explore and capture the implicit correspondence between the skirts part. Case (b) shows a similar result.
            </div>
        </td>
    </table>
    <br><br> -->
    
    <hr>
    <table align="center" width="980px">
        <tbody>
            <tr>
                <td>
                    <left>
                        <div class="center-div" align="center">
                            <h2>Acknowledgements</h2>
                        </div>
                        <div class="center-div" align="center"> 
                            The website is modified from this <a href="https://www.cs.cmu.edu/~wyuan1/pcn/">template</a>.
                        </div>
                    </left>
                </td>
            </tr>
        </tbody>
    </table>
    <br>



</body>

</html>
